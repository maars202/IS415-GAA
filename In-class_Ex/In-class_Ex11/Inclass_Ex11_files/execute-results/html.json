{
  "hash": "407bc5c9ec10938b7327a8031de1d6dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-Class Exercise 9 - week 11\"\nauthor: \"Maaruni\"\nexecute: \n  warning: false\ndate: 02/23/2024\ndate-modified: \"2024-03-18\"\n---\n\n\n# In class Exercise 9\n\n## Lecture Notes\n\n-   Model comparison and assessment both use only test data to get the metrics\n\n-   Recursive partitioning - how to link to single and multiple linear regression: cannot see and visualise the stuff similar to multiple linear regression\n\n-   Simple splitting rules, provides purities or impurities — concept in analytics foundation\n\n-   Response variable - predictive variable \n\n-   Categorical or numerical — not such a stringent required?\n\n-   Splitting rule — always mutually exclusive — model will take care of it - important to note when explaining the model - is it similar to hierarchical clustering where the groups might share commonalities and as we go down the graph they are more exclusively grouped and can only belong to one group? \n\n-   Regression tree and decision tree \n\n-   Categorical or numerical — since categorical for churn, — good for decision/classfication tree — using the classes\n\n-   Property price — average price of price — find out best split rules — use average to get the splitting rule - (important)Unlike single variable regression which uses all values and draws a line to get the relationship \n\n-   Continuous predictor — best split value — always mutually exclusive — like 12.3 is chosen as the split point for determining churn rate\n\n-   Must state explicitly whether the categorical variable is nominal or ordinal\n\n-   CART:\n\n    -   When the cart is allowed to stop splitting the groups, specify the process by stating these variables in the model:\n\n        -    the leaves of the tree should have size minimum leaf size — by default, minimum leaf size is 5\n\n        -   Max depth of tree \n\n-   The splitting at each level — it can be based on the same predictors from previous level\n\n-   If dataset is very small, the model might overfit — so recursive partitioning is inefficient \n\n-   Advanced recursive partitioning/random forest expands on this idea and prevents overfitting problem by (bagging method)creating large number of decision trees(an ensemble used together to get the overall prediction) — gets subsets from existing dataset and puts each subset into each of the trees - the trees might have overlaps and duplicates\n\n-   Bootstrap forest is another different method\n\n-   We cannot put x, y coordinates for the gwRF! - requires us to define the spatial properties — fixed or adaptive distance to get the variables that we can plug into the tree - not the x, y coordinates \n\n## **The Data**\n\nTwo data sets will be used in this model building exercise, they are:\n\n-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)\n\n-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*)\n\n## **Getting Started**\n\nThe code chunks below installs and launches these R packages into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, tidymodels, gtsummary,\n               rpart, rpart.plot, \n               ggstatsplot, performance, Metrics, tidyverse)\n```\n:::\n\n\nSome notes about the packages:\n\n-   SpatialML – only for random forests but not any other machine learning models\n\n-   tidymodels is useful for getting all the machine learning models! It is a wrapper that combines other packages such as yardstick for performance measurement, etc.\n\n-   rpart, rpart.plot are only in the in class exercise - used for recursive partitioning\n\n## \n\n## **Geospatial Data Wrangling**\n\n### **Importing geospatial data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_sf <- read_rds(\"../../data/data_week11/rds/HDB_resale2.rds\")\n```\n:::\n\n\nIt is in simple feature collection format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nresale_split <- initial_split(rs_sf, \n                              prop = 5/10,)\ntrain_data <- training(resale_split)\ntest_data <- testing(resale_split)\n```\n:::\n\n\nRsample is very useful and uses 2 steps:\n\n-   Initially splitting data (can be random or stratified) - by default is random\n\nSave train and test samples and reload them to be more memory efficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data_df <- train_data %>%\n  st_drop_geometry() %>%\n  as.data.frame()\n\ntest_data_df <- test_data %>%\n  st_drop_geometry() %>%\n  as.data.frame()\n```\n:::\n\n\nBoth of them will have one less column due to dropping geometry column.\n\n### Computing Correlation Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_sf1 <- rs_sf %>%\n  st_drop_geometry()\nggcorrmat(rs_sf1[, 2:17])\n```\n:::\n\n\nR function to convert to upper class\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoupper(\"floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_mlr <- lm(resale_price ~ FLOOR_AREA_SQM +                  STOREY_ORDER + REMAINING_LEASE_MTHS + PROX_CHAS +\n               PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n               PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL  +\n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n               WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(rs_mlr)\nsummary(rs_mlr)\n```\n:::\n\n\n### Revising mlr model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_df <- train_df %>%\n  select(-c(PROX_CHAS))\ntrain_sf <- train_sf %>%\n  select(-c(PROX_CHAS))\n\ntest_df <- test_df %>%\n  select(-c(PROX_CHAS))\ntest_sf <- test_sf %>%\n  select(-c(PROX_CHAS))\n```\n:::\n\n\nCHAS variable might not be be a signifcant contributor since its a Singapore policy that motivates health prices, NOT resale prices for house, as CHAS members get discounts for hospital charges, etc. So after removing it we will have 1 less variable.\n\nSo lets get rs_mlr without PROX_CHAS variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_mlr <- lm(resale_price ~ FLOOR_AREA_SQM +                  STOREY_ORDER + REMAINING_LEASE_MTHS +\n               PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n               PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL  +\n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n               WITHIN_1KM_PRISCH,\n                data=train_df)\nsummary(rs_mlr)\n```\n:::\n\n\n### Training Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data_sp <- as_Spatial(train_data)\ntrain_data_sp\n```\n:::\n\n\nThis code chunk extracts out the x, y coordinates of full, training and testing data sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- st_coordinates(rs_sf)\ncoords_train <- st_coordinates(train_sf)\ncoords_test <- st_coordinates(test_sf)\n```\n:::\n\n\n### \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}