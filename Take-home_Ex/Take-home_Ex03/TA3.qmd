---
title: "Take Home Exercise 3: Calibrating Hedonic Pricing Model for Airbnb Housing with GWR Method"
author: "Maaruni"
execute: 
  warning: false
date: 03/25/2024
---

## Take Home Assignment 3

We will be performing Geographically weighted regression (GWR) to derive estimates for airnb housing based on their other properties such as reviews per month. This will then be displayed on the shiny app for users to play around and get help in finding estimates so that they can choose which airbnb housings will suit their needs best. **Geographically weighted regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models for airbnb flats in Singapore by using GWR methods. The dependent variable is the airbnb prices of flats over several years. The independent variables are divided into either numerical and locational.

After all the outputs and analysis has been done as shown below, the shiny app will be displayed as follows for users to interact with:

## **UI Prototype**

::: panel-tabset
#### 1 - Prediction (Regression)

![](images/Screenshot%202024-03-24%20at%2011.25.13%20PM.png)

#### 2 - Reports

![](images/Screenshot%202024-03-24%20at%2011.42.08%20PM.png)
:::

The design of the app will allow users to seamlessly select between the 3 main models, namely simple, multiple and GWModel. Users will be able to generate the charts to see how explainable the model is with the R2 map. Alternatively they will also be able to choose the coefficient map to see how much the variables affect each region's estimates. For the three models they will be able to select from the relevant parameters to generate the plots.

Below these maps, there will be a table used to present the various publication quality reports for more detail inclined individuals to see for themselves the exact values and quality of the model such as with the use of sum of squares that indicates how much the prediction varies from actual values.

## **The Data**

Two data sets will be used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)

-   airbnb prices in csv format (i.e. listings*.csv*)

## **Getting Started**

Before we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.

The R packages needed for this exercise are as follows:

-   R package for building OLS and performing diagnostics tests

    -   [**olsrr**](https://olsrr.rsquaredacademy.com/)

-   R package for calibrating geographical weighted family of models

    -   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/)

-   R package for multivariate data visualisation and analysis

    -   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)

-   Spatial data handling

    -   **sf**

-   Attribute data handling

    -   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**

-   Choropleth mapping

    -   **tmap**

-   Ggplotly will also be used for 3D possible visualisations for a more interactive user experience. It is seen as relatively more mature package due to its ease of use in being able to download images from the interface itself, being able to zoom into details with the generated charts, etc.

-   Mapbox can also be used within ggplotly. However, one drawback is that it needs an api key to be generated for full usage. It has various advantages including more detailed view - aerial, etc. that can be useful for airbnb future rentors when they want to see nearby shops and such that may be available.

The code chunks below installs and launches these R packages into R environment.

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

## **Geospatial Data Wrangling**

### **Importing geospatial data**

The geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz = st_read(dsn = "../../data/geospatial/week10", layer = "MP14_SUBZONE_WEB_PL")
```

The report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called *mpsz* and it is a simple feature object. The geometry type is *multipolygon*. it is also important to note that mpsz simple feature object does not have EPSG information.

### **Updating CRS information**

The code chunk below updates the newly imported *mpsz* with the correct ESPG code (i.e. 3414)

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

After transforming the projection metadata, you can varify the projection of the newly transformed *mpsz_svy21* by using `st_crs()` of **sf** package.

The code chunk below will be used to varify the newly transformed *mpsz_svy21*.

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG: is indicated as *3414* now.

Next, you will reveal the extent of *mpsz_svy21* by using `st_bbox()` of sf package.

```{r}
st_bbox(mpsz_svy21) #view extent
```

## **Aspatial Data Wrangling of Airbnb**

### **Importing the aspatial data**

The *listings.csv* is in csv file format. The codes chunk below uses `read_csv()` function of **readr** package to import listings.csv into R as a tibble data frame called airbnb_resale.

```{r}
airbnb_resale = read_csv("data2/listings.csv")
glimpse(airbnb_resale)
```

After importing the data file into R, it is important for us to examine if the data file has been imported correctly.

The codes chunks below uses `glimpse()` to display the data structure of will do the job.

```{r}
head(airbnb_resale$latitude) #see the data in XCOORD column
```

```{r}
head(airbnb_resale$longitude) #see the data in XCOORD column
```

Next, `summary()` of base R is used to display the summary statistics of *cond_resale* tibble data frame.

### **Converting aspatial data frame into a sf object**

Currently, the airbnb_resale tibble data frame is aspatial. We will convert it to a **sf** object. The code chunk below converts airbnb_resale data frame into a simple feature data frame by using `st_as_sf()` of **sf** packages.

```{r}
airbnb_resale.sf <- st_as_sf(airbnb_resale,
                            coords = c("longitude", "latitude"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

Notice that `st_transform()` of **sf** package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).

Next, `head()` is used to list the content of airbnb_resale*.sf* object.

```{r}
head(airbnb_resale.sf)
```

Notice that the output is in point feature data frame.

## **Exploratory Data Analysis (EDA)**

In the section, you will learn how to use statistical graphics functions of **ggplot2** package to perform EDA.

### **EDA using statistical graphics**

We can plot the distribution of price by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

```{r}
ggplot(data=airbnb_resale.sf, aes(x=`price`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

The figure above reveals a left skewed distribution. This means that more airbnb units were transacted at relative lower prices.

Statistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called *LOG_SELLING_PRICE* by using a log transformation on the variable price.

```{r}
airbnb_resale.sf <- airbnb_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(price))
```

Now, you can plot the *LOG_SELLING_PRICE* using the code chunk below.

```{r}
ggplot(data=airbnb_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Now, the distribution is [**relatively less skewed**]{.underline} after the transformation.

### **Multiple Histogram Plots distribution of variables**

In this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using `ggarrange()` of [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/) package.

The code chunk below is used to create 12 histograms. Then, `ggarrange()` is used to organised these histogram into a 3 columns by 4 rows small multiple plot.

```{r}
NUM_REVIEWS <- ggplot(data=airbnb_resale.sf, aes(x= `number_of_reviews`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

MIN_NIGHTS <- ggplot(data=airbnb_resale.sf, aes(x= `minimum_nights`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

REVIEWS_PER_MONTH <- ggplot(data=airbnb_resale.sf, aes(x= `reviews_per_month`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

CALCULATED_HOST_LISTINGS_COUNT <- ggplot(data=airbnb_resale.sf, aes(x= `calculated_host_listings_count`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AVAILABILITY_365 <- ggplot(data=airbnb_resale.sf, aes(x= `availability_365`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

NUM_REVIEWS_LTM <- ggplot(data=airbnb_resale.sf, 
                               aes(x= `number_of_reviews_ltm`)) +
  geom_histogram(bins=20, color="black", fill="light blue")



ggarrange(NUM_REVIEWS, MIN_NIGHTS, REVIEWS_PER_MONTH, CALCULATED_HOST_LISTINGS_COUNT, AVAILABILITY_365, NUM_REVIEWS_LTM,  
          ncol = 3, nrow = 4)
```

### **Drawing Statistical Point Map**

Lastly, we want to reveal the geospatial distribution airbnb sale prices in Singapore. The map will be prepared by using **tmap** package.

First, we will turn on the interactive mode of tmap by using the code chunk below.

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
```

Next, the code chunks below is used to create an interactive point symbol map.

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(airbnb_resale.sf) +  
  tm_dots(col = "price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

Notice that [`tm_dots()`](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_symbols) is used instead of `tm_bubbles()`.

`set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom level to 11 and 14 respectively.

Before moving on to the next section, the code below will be used to turn R display into `plot` mode.

```{r}
tmap_mode("plot")
```

## **Hedonic Pricing Modelling in R**

In this section, you will learn how to building hedonic pricing models for airbnb rental units using [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm) of R base.

### **Simple Linear Regression Method**

First, we will build a simple linear regression model by using *PRICE* as the dependent variable and *reviews_per_month* as the independent variable.

```{r}
names(airbnb_resale)
```

```{r}
airbnb.slr <- lm(formula=price ~ reviews_per_month, data = airbnb_resale.sf)
```

`lm()` returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).

The functions `summary()` and `anova()` can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by `lm`.

```{r}
summary(airbnb.slr)
```

relationship is negatively proportional ==\> -6.719 x + 222.123 where x is reviews per month. This makes sense since the lower listing price could have led to a higher number of reviews indicates the listing became more popular. With this relationship only explaining 0.0004773, it does not seem very effective.

To visualise the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot’s geometry as shown in the code chunk below.

Figure below reveals that there are a few statistical outliers with relatively high selling prices.

```{r}
ggplot(data=airbnb_resale.sf,  
       aes(x=`reviews_per_month`, y=`price`)) +
  geom_point() +
  geom_smooth(method = lm)
```

### **Multiple Linear Regression Method**

#### Visualising the relationships of the independent variables

Before building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

Correlation matrix is commonly used to visualise the relationships between the independent variables. Beside the `pairs()` of R, there are many packages support the display of a correlation matrix. In this section, the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package will be used.

The code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in *airbnb_resale* data.frame.

numerical columns that are valid as independent variables.

```{r}
airbnb_cols = airbnb_resale %>% 
  select(10:12, 14:17)
airbnb_cols
```

```{r}
names(airbnb_cols)
```

Need to tidy data and impute certain columns with null values with the mean:

```{r}
print(sum(is.na(airbnb_cols)))
# replacing NA with each column's mean

# using colMeans()
mean_val <- colMeans(airbnb_cols,na.rm = TRUE)
 
# replacing NA with mean value of each column
for(i in colnames(airbnb_cols))
  airbnb_cols[,i][is.na(airbnb_cols[,i])] <- mean_val[i]

print(sum(is.na(airbnb_cols)))
```

```{r}
corrplot(cor(airbnb_cols), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Matrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

From the scatterplot matrix, it is clear that ***Freehold*** is highly correlated to ***LEASE_99YEAR***. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, ***LEASE_99YEAR*** is excluded in the subsequent model building.

### **Building a hedonic pricing model using multiple linear regression method**

The code chunk below using `lm()` to calibrate the multiple linear regression model.

```{r}
names(airbnb_cols)
```

```{r}
airbnb_resale.sf
```

```{r}
airbnb.mlr <- lm(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                data=airbnb_resale.sf)
summary(airbnb.mlr)

```

Finding which observations were deleted by model

```{r}
length(na.action(airbnb.mlr))
```

### **Preparing Publication Quality Table: olsrr method**

With reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.

Now, we are ready to calibrate the revised model by using the code chunk below.

```{r}
airbnb.mlr1 <- lm(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                data=airbnb_resale.sf)
ols_regress(airbnb.mlr1)
```

### **Preparing Publication Quality Table: gtsummary method**

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package provides an elegant and flexible way to create publication-ready summary tables in R.

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(airbnb.mlr1, intercept = TRUE)
```

With gtsummary package, model statistics can be included in the report by either appending them to the report table by using [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding as a table source note by using [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) as shown in the code chunk below.

```{r}
tbl_regression(airbnb.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

For more customisation options, refer to [Tutorial: tbl_regression](https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

#### Checking for multicolinearity

In this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called [**olsrr**](https://olsrr.rsquaredacademy.com/). It provides a collection of very useful methods for building better multiple linear regression models:

-   comprehensive regression output

-   residual diagnostics

-   measures of influence

-   heteroskedasticity tests

-   collinearity diagnostics

-   model fit assessment

-   variable contribution assessment

-   variable selection procedures

In the code chunk below, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) of **olsrr** package is used to test if there are sign of multicollinearity.

```{r}
ols_vif_tol(airbnb.mlr1)
```

Since the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

#### Test for Non-Linearity

In multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.

In the code chunk below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of **olsrr** package is used to perform linearity assumption test.

```{r}
ols_plot_resid_fit(airbnb.mlr1)
```

The figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.

#### Test for Normality Assumption

Lastly, the code chunk below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of *olsrr* package to perform normality assumption test.

```{r}
ols_plot_resid_hist(airbnb.mlr1)
```

The figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.

If you prefer formal statistical test methods, the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of **olsrr** package can

```{r}
ols_test_normality(airbnb.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

Multiple Linear Regression model created and can be used for prediction.

These results will be displayed on Shiny app.

#### Testing for Spatial Autocorrelation

The hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert *airbnb_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}
airbnb.mlr.output <- as.data.frame(airbnb.mlr1$residuals)
```

Next, we will join the newly created data frame with *condo_resale.sf* object.

```{r}
head(airbnb_resale.sf)
```

```{r}
#airbnb_resale.sf$index
nrow(airbnb_resale.sf)

# Convert rownames to a column
airbnb_resale.sf<- rownames_to_column(airbnb_resale.sf, var = "row_index")

# View the modified dataset
print(airbnb_resale.sf)
```

```{r}
airbnb_resale.sf <- airbnb_resale.sf[!airbnb_resale.sf$row_index %in% na.action(airbnb.mlr), ]
```

```         
```

```{r}
length(na.action(airbnb.mlr))
#na.action(airbnb.mlr)
```

```{r}
#length(airbnb.mlr1$na.action)
```

```{r}
length(airbnb.mlr1$residuals)
```

```{r}
airbnb_resale.res.sf <- cbind(airbnb_resale.sf, 
                        airbnb.mlr1$residuals) %>%
rename(`MLR_RES` = `airbnb.mlr1.residuals`)
```

Next, we will convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

The code chunk below will be used to perform the data conversion process.

```{r}
airbnb_resale.sp <- as_Spatial(airbnb_resale.res.sf)
airbnb_resale.sp
```

```{r}
tmap_mode("view")
```

```{r}
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(airbnb_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

```{r}
tmap_mode("plot")
```

```{r}
nb <- dnearneigh(coordinates(airbnb_resale.sp), 0, 1, longlat = FALSE)
summary(nb)
```

```{r}
#lm.morantest(condo.mlr1, nb_lw)
```

```{r}
#nb_lw <- nb2listw(nb, style = 'W')
#summary(nb_lw)
```

```{r}
bw.fixed <- bw.gwr(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                   data=airbnb_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

```{r}
gwr.fixed <- gwr.basic(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                   data=airbnb_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

```{r}
gwr.fixed
```

### **13.9.2 Building Adaptive Bandwidth GWR Model**

In this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.

#### 13.9.2.1 Computing the adaptive bandwidth

Similar to the earlier section, we will first use `bw.gwr()` to determine the recommended data point to use.

The code chunk used look very similar to the one used to compute the fixed bandwidth except the `adaptive` argument has changed to **TRUE**.

```{r}
bw.adaptive <- bw.gwr(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                   data=airbnb_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

#### 13.9.2.2 Constructing the adaptive bandwidth gwr model

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.

```{r}
gwr.adaptive <- gwr.basic(formula = price ~ minimum_nights + number_of_reviews    + 
                  reviews_per_month + calculated_host_listings_count + availability_365 +
                  number_of_reviews_ltm, 
                   data=airbnb_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

```{r}
gwr.adaptive
```

This shows that adaptive is much better!.

### **13.9.3 Visualising GWR Output**

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.

-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.

-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.

-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called **SDF** of the output list.

### **13.9.4 Converting SDF into *sf* data.frame**

To visualise the fields in **SDF**, we need to first covert it into **sf** data.frame by using the code chunk below.

```{r}
airbnb_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
airbnb_resale.sf.adaptive.svy21 <- st_transform(airbnb_resale.sf.adaptive, 3414)
airbnb_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
airbnb_resale.sf.adaptive <- cbind(airbnb_resale.res.sf, as.matrix(gwr.adaptive.output))
```

```{r}
glimpse(airbnb_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

### **13.9.5 Visualising local R2**

The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(airbnb_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```

```{r}
tmap_mode("plot")
```

### **13.9.6 Visualising coefficient estimates**

The code chunks below is used to create an interactive point symbol map.

```{r}
names(airbnb_resale.sf.adaptive)
```

```{r}

sf_use_s2(TRUE)
tmap_mode("view")
number_of_reviews_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(airbnb_resale.sf.adaptive) +  
  tm_dots(col = "number_of_reviews_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

number_of_reviews_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(airbnb_resale.sf.adaptive) +  
  tm_dots(col = "number_of_reviews_TV",
          size = 0.15,
          border.col = "gray60",
          border.lwd = 1, palette = "YlGn") +
  tm_view(set.zoom.limits = c(11,14)) 

tmap_arrange(number_of_reviews_SE, number_of_reviews_TV,  asp=1, ncol=2, sync = TRUE)
#tm_fill/tm_borders/tm_polygons.
#Run All Chunks AboveRun Current Chunk
#number_of_reviews_TV

```

```{r}
airbnb_resale.sf.adaptive$AREA_SQM_TV
names(airbnb_resale.sf.adaptive)
#airbnb_resale.sf.adaptive$Local_R2[:10]
```

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(airbnb_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

## Reflections/Personal Take

This hands on exercise was insightful in helping form a more solid understanding of what features to use for end users. As human users, it will be difficult for them to calculate spatially weighted relationships with the naked eye and this app will definitely be useful in helping them understand the relationship between price and all the various variables easily. It also makes the regression models more explainable.

## **13.10 Reference**

-   <https://r4gdsa.netlify.app/chap13.html>
