---
title: "Take Home Exercise 2"
author: "Maaruni"
execute: 
  warning: false
date: 02/09/2024
editor: 
  markdown: 
    wrap: sentence
---

# Overview

## Setting the Scene

Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions.
It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes.
In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths.
Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases.
However, in 2023, Taiwan recorded 26703 dengue fever cases.
Figure below reveals that more than 25,000 cases were reported at Tainan City.

Figure 2 and 3 below reveal that more than 80% of the reported dengue fever cases occurred in the month August-November 2023 and epidemiology week 31-50.

## Objectives

As a curious geospatial analytics green horn, you are interested to discover:

if the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.
If the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

## The Task

The specific tasks of this take-home exercise are as follows:

Using appropriate function of sf and tidyverse, preparing the following geospatial data layer: a study area layer in sf polygon features.
It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.
a dengue fever layer within the study area in sf point features.
The dengue fever cases should be confined to epidemiology week 31-50, 2023.
a derived dengue fever layer in spacetime s3 class of sfdep.
It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.
Using the extracted data, perform global spatial autocorrelation analysis.
Using the extracted data, perform local spatial autocorrelation analysis.
Using the extracted data, perform emerging hotspot analysis.
Describe the spatial patterns revealed by the analysis above.

## The Data

For the purpose of this take-home exercise, two data sets are provided, they are:

TAIWAN_VILLAGE_2020, a geospatial data of village boundary of Taiwan.
It is in ESRI shapefile format.
The data is in Taiwan Geographic Coordinate System.
(Source: Historical map data of the village boundary: TWD97 longitude and latitude)

Dengue_Daily.csv, an aspatial data of reported dengue cases in Taiwan since 1998.
(Source: Dengue Daily Confirmed Cases Since 1998. Below are selected fields that are useful for this study:

發病日: Onset date 最小統計區中心點X: x-coordinate 最小統計區中心點Y: y-coordinate Both data sets have been uploaded on eLearn.
Students are required to download them from eLearn.

# Getting Started

## Data Acquisition

|                                |                                                                           |
|----------------------|-------------------------------------------------|
| [**Dataset Name**]{.underline} | [**Source**]{.underline}                                                  |
| TAIWAN_VILLAGE_2020            | Historical map data of the village boundary: TWD97 longitude and latitude |
| Dengue_Daily.csv               | Dengue Daily Confirmed Cases Since 1998                                   |

## Installing and Loading Packages

Lets install the relevant R libraries needed using pacman.

```{r}
pacman::p_load(sf, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr, spacetime)
```

### Setting Important Configurations

```{r}
#| eval: false
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

```{r}
#| eval: false
currentdirec = list.files(path="../../data/takehomeassigment2/geospatial", pattern=NULL, all.files=FALSE, 
    full.names=FALSE)
currentdirec
```

Reading the grab aspatial data:

```{r}
#| eval: false
dengue_df <- read_csv("../../data/takehomeassigment2/aspatial/Dengue_Daily.csv")
dengue_df
```

Getting Columns of dengue_df:

```{r}
#| eval: false
names(dengue_df)
```

Lets print a summary of dengue_df to find the data distribution and other information:

```{r}
#| eval: false
summary(dengue_df[])
```

We need to retrieve the coastal outline of Taiwan village so that we are able to fetch the dengue infections specifically within these boundaries.
According to <https://epsg.io/3826>, the EPSG code for taiwan to be used as the coordinate system is 3826.
Thus, the data has been projected to Taiwan's coordinate system using 3826.
However since the data is already projected to Taiwan's coordinate system we do not need to do this

```{r}
#| eval: false
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE")
taiwan_sf
```

```{r}
#| eval: false
plot(taiwan_sf)
```

```{r}
#| eval: false
# find location of missing values
print("Position of missing values ")
which(is.na(dengue_df))
 
# count total missing values 
print("Count of total missing values  ")
sum(is.na(dengue_df))


colSums(is.na(dengue_df))
```

::: callout-tip
Interesting observation I had here was how all the columns had no missing values and yet the map was not being plotted.
Then, I realised it was due to the null values being presented as "None" string instead of the numerical value such as the example below.
:::

```{r}
#| eval: false
dengue_df[6,]$最小統計區中心點X
```

Lets filter out all the None values to get valid x and y coordinates.
最小統計區中心點X, 最小統計區中心點Y

```{r}
#| eval: false
dengue_df_filtered <- filter(dengue_df, 最小統計區中心點X != "None" & 最小統計區中心點Y != "None")
head(dengue_df_filtered)
```

|     |
|-----|
|     |
|     |

We need to filter the onset date column by epiweek to get epiweeks 31 to 50 of 2023.
According to <https://www.cmmcp.org/sites/g/files/vyhlif2966/f/uploads/epiweekcalendar2023.pdf>, the start date of epiweek 31 in 2023 is 30-07-2023 and end date of epiweek 50 in 2023 is 16-12-2023.

```{r}
#| eval: false
dengue_df_filtered$epiweek = epiweek(dengue_df_filtered$發病日)
names(dengue_df_filtered)
```

```{r}
#| eval: false
start_date = "2023-07-30"
end_date = "2023-12-16"
dengue_df_filtered = filter(dengue_df_filtered, 發病日 >= start_date & 發病日 <= end_date)
dengue_df_filtered
```

After filtering valid points, lets convert the latitude and longitude from wsg84 to TWD97 as it conforms Taiwan's coordinate system for easier analysis and for us to join it with the taiwan_sf later.

<!--#  3826 https://epsg.io/transform#s_srs=3826&t_srs=4326&ops=3830&x=NaN&y=NaN -->

```{r}
#| eval: false
dengue_df_filtered <- st_as_sf(dengue_df_filtered, 
                       coords = c("最小統計區中心點X","最小統計區中心點Y"),
                       crs=3826) %>%
st_transform(crs = 3826)
glimpse(dengue_df_filtered)
```

Let us save this combined dataframe to RDS so that we will not need to repeat the above steps.
Replace filepath with the directory you would like to save the rds at.

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
write_rds(dengue_df_filtered, filepath) 
```

Reload the data from the file path from here for subsequent steps:

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
head(dengue_df) 
```

# Analysis

```{r}
pacman::p_load(sf, sfdep, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr)
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

Let's read the taiwan polygon data.

```{r}
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE")
head(taiwan_sf)
```

Lets read the previously saved filtered and saved data:

```{r}
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
head(dengue_df) 
```

These are the columns in chinese for dengue_df.
It is inconvenient to use these.
Thus, let's convert them to English.

```{r}
print(names(dengue_df))
names(dengue_df)[1] = "Day_of_onset"
names(dengue_df)[2] = "Day_of_judgement"
names(dengue_df)[3] = "Day_of_report"
names(dengue_df)[4] = "Gender"
names(dengue_df)[5] = "Age_group"

names(dengue_df)[6] = "County_and_city_of_residence"
names(dengue_df)[7] = "TOWNNAME"
names(dengue_df)[8] = "VILLNAME"
names(dengue_df)[9] = "Minimum_statistical_area"
names(dengue_df)[10] = "First_level_statistical_area"

names(dengue_df)[11] = "Secondary_level_statistical_area"
names(dengue_df)[12] = "Infected_counties_and_cities"
names(dengue_df)[13] = "Infected_towns"
names(dengue_df)[14] = "Infect_the_village"
names(dengue_df)[15] = "Whether_to_immigrate_from_abroad"

names(dengue_df)[16] = "Infected_countries"
names(dengue_df)[17] = "Number_of_cases"
names(dengue_df)[18] = "Residential_village_code"
names(dengue_df)[19] = "Infected_Village_Code"
names(dengue_df)[20] = "Serotype"

names(dengue_df)[21] = "Ministry_of_Interior_county"
names(dengue_df)[22] = "Home_Office_Township_Code_of_Residence"
names(dengue_df)[23] = "Ministry_of_Interior"
names(dengue_df)[24] = "Home_Office_Infection_Township_Code"
```

## Data distribution for columns

::: panel-tabset
#### Age

```{r}
#| eval: false
g <- ggplot(dengue_df_2, aes(年齡層))  
p <-  g + geom_bar() + ggtitle("Count of Entries by Age") +  xlab("Age") + ylab("Count")

ggplotly(p) 
```

#### Infected_counties_and_cities

```{r}
#| eval: false
g <- ggplot(dengue_df, aes(Infected_counties_and_cities))  
p <-  g + geom_bar() + ggtitle("Count of Entries by cities") +  xlab("Infected_counties_and_cities") + ylab("Count")

ggplotly(p) 
```

\
:::

Since we only need the polygons from D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan, we we will filter our polygons from these regions only:

```{r}
taiwan_sf_filtered <- filter(taiwan_sf, TOWNID %in% c('D01', 'D02', 'D04', 'D06', 'D07', 'D08', 'D32', 'D39'))
head(taiwan_sf_filtered)
```

Lets look at the columns present in Taiwan_sf:

```{r}
names(taiwan_sf_filtered)
```

Since we need to do an analysis of the count of cases at village level and by weeks, let's group by village town and week to get the count of cases according to village and weeks.
Since multiple villages exist in a town and both town and village fields exist in taiwan sf, we can group by with townname as well.

```{r}
dengue_df_count = dengue_df %>% group_by(VILLNAME, TOWNNAME, epiweek) %>%
  summarise(total_count = n())

dengue_df_count = st_drop_geometry(dengue_df_count)
head(dengue_df_count)
```

We can combine villname and townname as it is representative of the region and can be used as a single column later.

```{r}
dengue_df_count$VILLTOWN = paste(dengue_df_count$VILLNAME, dengue_df_count$TOWNNAME)
taiwan_sf_filtered$VILLTOWN = paste(taiwan_sf_filtered$VILLNAME, taiwan_sf_filtered$TOWNNAME)
```

We can now ungroup all the times and keep only VILLNAME, TOWNNAME, epiweek and the derived count column for the dengue cases in each region at that epiweek.

```{r}
dengue_df_count2 = dengue_df_count %>%
            ungroup() %>%
  select(3,4, 5) 
names(dengue_df_count2)[1] = "epiweek"
head(dengue_df_count2)
```

Before adding in entries for missing observations in taiwan sf, lets observe how it initially looks like:

![](vacrate.gif)

As we can see there are many missing regions with no observations according to dengue_df.
We need to creating empty observations with count 0 so that we are able to create the spacetime cube later for spatiotemporal analysis.

```{r}
#for each town check if the week exists is not then add row for that week for that reach with count 0 
total_invalids = 0
for(i in 1:nrow(taiwan_sf_filtered))
{
  region = taiwan_sf_filtered$VILLTOWN[i]
  #cat("region: ", region)
  for(j in 31:50){
    if (nrow(dengue_df_count2[dengue_df_count2$epiweek == j & dengue_df_count2$VILLTOWN == region, ]) == 0){
    matching_week_region = c(epiweek = j, total_count = 0, VILLTOWN = region)
    dengue_df_count2 = rbind(dengue_df_count2,matching_week_region) 
    total_invalids = total_invalids + 1
    }
  }
  #print(total_invalids)
}
print(total_invalids)
```

Since the epiweek and total_count are in integer format we need to cast them to integer for our analysis.

```{r}
dengue_df_count2['epiweek'] <- as.integer(unlist(dengue_df_count2['epiweek']))
dengue_df_count2['total_count'] <- as.integer(unlist(dengue_df_count2['total_count']))
```

Lets add the attributes of dengue_df_count2 to the taiwan sf spatial dataset using a left join:

```{r}
dengue_df_combined = left_join(taiwan_sf_filtered, dengue_df_count2)
dengue_df_combined <- dengue_df_combined %>%
  select(11, 12, 13)
names(dengue_df_combined)[2] = "epiweek"
head(dengue_df_combined)
```

We can look at the distribution of data for each region and check if each village has 20 weeks worth of observations.

```{r}
dengue_df_distribution = dengue_df_combined %>% group_by(VILLTOWN) %>%
  summarise(total_count_weeks = n())
dengue_df_distribution
```

Nice, all of them have 20 observations for weeks 31 to 50 of year 2023.

Let's analyse the map of a single week 33.

```{r}
week33 = dengue_df_combined[dengue_df_combined$epiweek == 33, ]
tmap_mode("plot")
tm_shape(week33) +
  tm_fill("total_count") +
  tm_borders()
```

Lets create an animation of the changing dengue cases over the 20 weeks.

```{r}
vacrate_anim <-
  tm_shape(dengue_df_combined) + tm_fill("total_count",
            palette = "Purples") +
    tm_borders(lwd = 0.1) +
  tm_facets(along = "epiweek", free.coords = FALSE)
```

This is the animated object.

```{r}
vacrate_anim
```

We can save this object to a gif file so that it can be viewed in an animated fashion.

```{r}
tmap_animation(vacrate_anim, filename = "vacrate2.gif", delay = 100, width = 1280, height = 720, scale = 2)
```

![](vacrate2.gif)

```{r}
dengue_df_combined_withgeometry = dengue_df_combined
dengue_df_combined = st_drop_geometry(dengue_df_combined)
head(dengue_df_combined)
```

Lets create a spacetime object

```{r}
bos = spacetime(dengue_df_combined, taiwan_sf_filtered,
                .loc_col = "VILLTOWN",
                .time_col = "epiweek")
head(bos)
```

Lets check if the spacetime object created is valid:

```{r}
is_spacetime_cube(bos)
```

Nice, it it valid with 20 time periods \* 258 regions.

```{r}
bos = complete_spacetime_cube(bos)
head(bos)
```

Lets get the neighbors and weights around the region.

```{r}
dengue_nb <- bos %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
head(dengue_nb)
```

If we look at the neighbors of 1 and its neighbors, we see that they all exist in their respective nb column which stands for neighbors and includes itself:

```{r}
cat("neighbors of region 1: ")
print( dengue_nb[1, ]$nb)
cat("neighbors of region 6: ")
print( dengue_nb[6, ]$nb)

cat("neighbors of region 118: ")
print( dengue_nb[118, ]$nb)
cat("neighbors of region 160: ")
print( dengue_nb[160, ]$nb)
```

```{r}
gi_stars <- dengue_nb %>% 
  group_by(epiweek) %>% 
  mutate(gi_star = local_gstar_perm(
    total_count, nb, wt)) %>% 
  tidyr::unnest(gi_star)


```

## **Computing Gi\***

isolating only one week :

```{r}
dengue_nb_week31 = filter(dengue_nb, dengue_nb$epiweek == 31)
head(dengue_nb_week31)
```

some of them have no neighbors

```{r}
unlist(dengue_nb_week31[1, ]$nb)
```

```{r}
dengue_nb_week31[187, ]$nb
lengths(dengue_nb_week31[187, ]$nb)
print("--------------")
#card(dengue_nb_week31[1, ]$nb[1])
print("--------------")
#card(dengue_nb_week31[187, ]$nb)

```

```{r}
gi_star = local_gstar_perm( dengue_nb_week31, dengue_nb_week31$nb, dengue_nb_week31$wt)
gi_star
```

method 2

```{r}
#wm_idw <- dengue_df_combined %>%
#  mutate(nb = st_contiguity(geometry),
#         wts = st_inverse_distance(nb, geometry,
#                                   scale = 1,
#                                   alpha = 1),
#         .before = 1)
```

this might be useful for points that have missing towns:

```         
left_join(dfx, dfy, join_by(closest(a < b))) # similar to above, but only take the closest match
```

::: callout-note
Planning:

-   use statistical means to get distribution and see which one best - identify unique patterns and write reflection on them

<!-- -->

-   add the cold and hot spots as separate layers for users to interact with

-   facet plots

-   make ranges for plots similar when comparing plots side by side
:::

# References

-   <https://r4gdsa.netlify.app/chap04.html>

-   <https://r4gdsa.netlify.app/chap07.html>
