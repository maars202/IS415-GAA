---
title: "Take Home Exercise 2"
author: "Maaruni"
execute: 
  warning: false
date: 02/16/2024
editor: 
  markdown: 
    wrap: sentence
---

# Overview

## Setting the Scene

Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions.
It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes.
In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths.
Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases.
However, in 2023, Taiwan recorded 26703 dengue fever cases.
Figure below reveals that more than 25,000 cases were reported at Tainan City.

Figure 2 and 3 below reveal that more than 80% of the reported dengue fever cases occurred in the month August-November 2023 and epidemiology week 31-50.

## Objectives

As a curious geospatial analytics green horn, you are interested to discover:

if the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.
If the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

## The Task

The specific tasks of this take-home exercise are as follows:

Using appropriate function of sf and tidyverse, preparing the following geospatial data layer: a study area layer in sf polygon features.
It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.
a dengue fever layer within the study area in sf point features.
The dengue fever cases should be confined to epidemiology week 31-50, 2023.
a derived dengue fever layer in spacetime s3 class of sfdep.
It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.
Using the extracted data, perform global spatial autocorrelation analysis.
Using the extracted data, perform local spatial autocorrelation analysis.
Using the extracted data, perform emerging hotspot analysis.
Describe the spatial patterns revealed by the analysis above.

## The Data

For the purpose of this take-home exercise, two data sets are provided, they are:

TAIWAN_VILLAGE_2020, a geospatial data of village boundary of Taiwan.
It is in ESRI shapefile format.
The data is in Taiwan Geographic Coordinate System.
(Source: Historical map data of the village boundary: TWD97 longitude and latitude)

Dengue_Daily.csv, an aspatial data of reported dengue cases in Taiwan since 1998.
(Source: Dengue Daily Confirmed Cases Since 1998. Below are selected fields that are useful for this study:

發病日: Onset date 最小統計區中心點X: x-coordinate 最小統計區中心點Y: y-coordinate Both data sets have been uploaded on eLearn.
Students are required to download them from eLearn.

# Getting Started

## Data Acquisition

|                                |                                                                           |
|----------------------|-------------------------------------------------|
| [**Dataset Name**]{.underline} | [**Source**]{.underline}                                                  |
| TAIWAN_VILLAGE_2020            | Historical map data of the village boundary: TWD97 longitude and latitude |
| Dengue_Daily.csv               | Dengue Daily Confirmed Cases Since 1998                                   |

## Installing and Loading Packages

Lets install the relevant R libraries needed using pacman.

```{r}
pacman::p_load(sf, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr, spacetime)
```

### Setting Important Configurations

```{r}
#| eval: false
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

```{r}
#| eval: false
currentdirec = list.files(path="../../data/takehomeassigment2/geospatial", pattern=NULL, all.files=FALSE, 
    full.names=FALSE)
currentdirec
```

Reading the grab aspatial data:

```{r}
#| eval: false
dengue_df <- read_csv("../../data/takehomeassigment2/aspatial/Dengue_Daily.csv")
dengue_df
```

Getting Columns of dengue_df:

```{r}
#| eval: false
names(dengue_df)
```

Lets print a summary of dengue_df to find the data distribution and other information:

```{r}
#| eval: false
summary(dengue_df[])
```

We need to retrieve the coastal outline of Taiwan village so that we are able to fetch the dengue infections specifically within these boundaries.
According to <https://epsg.io/3826>, the EPSG code for taiwan to be used as the coordinate system is 3826.
Thus, the data has been projected to Taiwan's coordinate system using 3826.
However since the data is already projected to Taiwan's coordinate system we do not need to do this

```{r}
#| eval: false
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE")
taiwan_sf
```

```{r}
#| eval: false
plot(taiwan_sf)
```

```{r}
#| eval: false
# find location of missing values
print("Position of missing values ")
which(is.na(dengue_df))
 
# count total missing values 
print("Count of total missing values  ")
sum(is.na(dengue_df))


colSums(is.na(dengue_df))
```

::: callout-tip
Interesting observation I had here was how all the columns had no missing values and yet the map was not being plotted.
Then, I realised it was due to the null values being presented as "None" string instead of the numerical value such as the example below.
:::

```{r}
#| eval: false
dengue_df[6,]$最小統計區中心點X
```

Lets filter out all the None values to get valid x and y coordinates.
最小統計區中心點X, 最小統計區中心點Y

```{r}
#| eval: false
dengue_df_filtered <- filter(dengue_df, 最小統計區中心點X != "None" & 最小統計區中心點Y != "None")
head(dengue_df_filtered)
```

|     |
|-----|
|     |
|     |

We need to filter the onset date column by epiweek to get epiweeks 31 to 50 of 2023.
According to <https://www.cmmcp.org/sites/g/files/vyhlif2966/f/uploads/epiweekcalendar2023.pdf>, the start date of epiweek 31 in 2023 is 30-07-2023 and end date of epiweek 50 in 2023 is 16-12-2023.

```{r}
#| eval: false
dengue_df_filtered$epiweek = epiweek(dengue_df_filtered$發病日)
names(dengue_df_filtered)
```

```{r}
#| eval: false
start_date = "2023-07-30"
end_date = "2023-12-16"
dengue_df_filtered = filter(dengue_df_filtered, 發病日 >= start_date & 發病日 <= end_date)
dengue_df_filtered
```

After filtering valid points, lets convert the latitude and longitude from wsg84 to TWD97 as it conforms Taiwan's coordinate system for easier analysis and for us to join it with the taiwan_sf later.

<!--#  3826 https://epsg.io/transform#s_srs=3826&t_srs=4326&ops=3830&x=NaN&y=NaN -->

```{r}
#| eval: false
dengue_df_filtered <- st_as_sf(dengue_df_filtered, 
                       coords = c("最小統計區中心點X","最小統計區中心點Y"),
                       crs=3826) %>%
st_transform(crs = 3826)
glimpse(dengue_df_filtered)
```

Let us save this combined dataframe to RDS so that we will not need to repeat the above steps.
Replace filepath with the directory you would like to save the rds at.

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
write_rds(dengue_df_filtered, filepath) 
```

Reload the data from the file path from here for subsequent steps:

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
head(dengue_df) 
```

# Analysis

```{r}
pacman::p_load(sf, sfdep, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr)
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

Let's read the taiwan polygon data.

```{r}
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE")
head(taiwan_sf)
```

Lets read the previously saved filtered and saved data:

```{r}
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
head(dengue_df) 
```

These are the columns in chinese for dengue_df.
It is inconvenient to use these.
Thus, let's convert them to English.

```{r}
print(names(dengue_df))
names(dengue_df)[1] = "Day_of_onset"
names(dengue_df)[2] = "Day_of_judgement"
names(dengue_df)[3] = "Day_of_report"
names(dengue_df)[4] = "Gender"
names(dengue_df)[5] = "Age_group"

names(dengue_df)[6] = "County_and_city_of_residence"
names(dengue_df)[7] = "TOWNNAME"
names(dengue_df)[8] = "VILLNAME"
names(dengue_df)[9] = "Minimum_statistical_area"
names(dengue_df)[10] = "First_level_statistical_area"

names(dengue_df)[11] = "Secondary_level_statistical_area"
names(dengue_df)[12] = "Infected_counties_and_cities"
names(dengue_df)[13] = "Infected_towns"
names(dengue_df)[14] = "Infect_the_village"
names(dengue_df)[15] = "Whether_to_immigrate_from_abroad"

names(dengue_df)[16] = "Infected_countries"
names(dengue_df)[17] = "Number_of_cases"
names(dengue_df)[18] = "Residential_village_code"
names(dengue_df)[19] = "Infected_Village_Code"
names(dengue_df)[20] = "Serotype"

names(dengue_df)[21] = "Ministry_of_Interior_county"
names(dengue_df)[22] = "Home_Office_Township_Code_of_Residence"
names(dengue_df)[23] = "Ministry_of_Interior"
names(dengue_df)[24] = "Home_Office_Infection_Township_Code"
```

## Data distribution for columns

::: panel-tabset
#### Age

```{r}
#| eval: false
g <- ggplot(dengue_df_2, aes(年齡層))  
p <-  g + geom_bar() + ggtitle("Count of Entries by Age") +  xlab("Age") + ylab("Count")

ggplotly(p) 
```

#### Infected_counties_and_cities

```{r}
#| eval: false
g <- ggplot(dengue_df, aes(Infected_counties_and_cities))  
p <-  g + geom_bar() + ggtitle("Count of Entries by cities") +  xlab("Infected_counties_and_cities") + ylab("Count")

ggplotly(p) 
```

\
:::

Since we only need the polygons from D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan, we we will filter our polygons from these regions only:

```{r}
taiwan_sf_filtered <- filter(taiwan_sf, TOWNID %in% c('D01', 'D02', 'D04', 'D06', 'D07', 'D08', 'D32', 'D39'))
head(taiwan_sf_filtered)
```

Lets look at the columns present in Taiwan_sf:

```{r}
names(taiwan_sf_filtered)
```

Since we need to do an analysis of the count of cases at village level and by weeks, let's group by village town and week to get the count of cases according to village and weeks.
Since multiple villages exist in a town and both town and village fields exist in taiwan sf, we can group by with townname as well.

```{r}
dengue_df_count = dengue_df %>% group_by(VILLNAME, TOWNNAME, epiweek) %>%
  summarise(total_count = n())

dengue_df_count = st_drop_geometry(dengue_df_count)
head(dengue_df_count)
```

We can combine villname and townname as it is representative of the region and can be used as a single column later.

```{r}
dengue_df_count$VILLTOWN = paste(dengue_df_count$VILLNAME, dengue_df_count$TOWNNAME)
taiwan_sf_filtered$VILLTOWN = paste(taiwan_sf_filtered$VILLNAME, taiwan_sf_filtered$TOWNNAME)
```

We can now ungroup all the times and keep only VILLNAME, TOWNNAME, epiweek and the derived count column for the dengue cases in each region at that epiweek.

```{r}
dengue_df_count2 = dengue_df_count %>%
            ungroup() %>%
  select(3,4, 5) 
names(dengue_df_count2)[1] = "epiweek"
head(dengue_df_count2)
```

Before adding in entries for missing observations in taiwan sf, lets observe how it initially looks like:

![](vacrate.gif)

As we can see there are many missing regions with no observations according to dengue_df.
We need to creating empty observations with count 0 so that we are able to create the spacetime cube later for spatiotemporal analysis.

```{r}
#for each town check if the week exists is not then add row for that week for that reach with count 0 
total_invalids = 0
for(i in 1:nrow(taiwan_sf_filtered))
{
  region = taiwan_sf_filtered$VILLTOWN[i]
  #cat("region: ", region)
  for(j in 31:50){
    if (nrow(dengue_df_count2[dengue_df_count2$epiweek == j & dengue_df_count2$VILLTOWN == region, ]) == 0){
    matching_week_region = c(epiweek = j, total_count = 0, VILLTOWN = region)
    dengue_df_count2 = rbind(dengue_df_count2,matching_week_region) 
    total_invalids = total_invalids + 1
    }
  }
  #print(total_invalids)
}
print(total_invalids)
```

Since the epiweek and total_count are in integer format we need to cast them to integer for our analysis.

```{r}
dengue_df_count2['epiweek'] <- as.integer(unlist(dengue_df_count2['epiweek']))
dengue_df_count2['total_count'] <- as.integer(unlist(dengue_df_count2['total_count']))
```

Lets add the attributes of dengue_df_count2 to the taiwan sf spatial dataset using a left join:

```{r}
dengue_df_combined = left_join(taiwan_sf_filtered, dengue_df_count2)
dengue_df_combined <- dengue_df_combined %>%
  select(11, 12, 13)
names(dengue_df_combined)[2] = "epiweek"
head(dengue_df_combined)
```

We can look at the distribution of data for each region and check if each village has 20 weeks worth of observations.

```{r}
dengue_df_distribution = dengue_df_combined %>% group_by(VILLTOWN) %>%
  summarise(total_count_weeks = n())
dengue_df_distribution
```

Nice, all of them have 20 observations for weeks 31 to 50 of year 2023.

Let's analyse the map of a single week 33.

```{r}
week33 = dengue_df_combined[dengue_df_combined$epiweek == 33, ]
tmap_mode("plot")
tm_shape(week33) +
  tm_fill("total_count") +
  tm_borders()
```

Let us try to visualise the data using equal interval and equal quantile classifcations:

```{r}
equal <- tm_shape(week33) +
  tm_fill("total_count",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(week33) +
  tm_fill("total_count",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

This shows that the data unevenly distributed with most regions having around 1 dengue case.

Lets create an animation of the changing dengue cases over the 20 weeks.

```{r}
vacrate_anim <-
  tm_shape(dengue_df_combined) + tm_fill("total_count",
            palette = "Purples") +
    tm_borders(lwd = 0.1) +
  tm_facets(along = "epiweek", free.coords = FALSE)
```

This is the animated object.

```{r}
vacrate_anim
```

We can save this object to a gif file so that it can be viewed in an animated fashion.

```{r}
tmap_animation(vacrate_anim, filename = "vacrate2.gif", delay = 100, width = 1280, height = 720, scale = 2)
```

![](vacrate2.gif)

```{r}
#dengue_df_combined_withgeometry = dengue_df_combined
#dengue_df_combined = st_drop_geometry(dengue_df_combined)
#head(dengue_df_combined)
```

# **Creating a Time Series Cube**

Let's create a spacetime cube with spacetime() function from sfdep:

```{r}
bos = spacetime(dengue_df_combined, taiwan_sf_filtered,
                .loc_col = "VILLTOWN",
                .time_col = "epiweek")
head(bos)
```

Lets check if the spacetime object created is valid:

```{r}
is_spacetime_cube(bos)
```

Nice, it it valid with 20 time periods \* 258 regions.

Let us double check the number of observations with this function:

```{r}
bos = complete_spacetime_cube(bos)
head(bos)
```

```{r}
valid_rows = dplyr::count(bos, epiweek, VILLTOWN)
head(valid_rows)
```

## **Computing Gi\***

Lets get the neighbors and inverse distance weights around the region.
Note that the region itself will be included as a neighbor:

```{r}
dengue_nb <- bos %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
head(dengue_nb)
```

If we look at the neighbors of 1 and its neighbors, we see that they all exist in their respective nb column which stands for neighbors and includes itself:

```{r}
cat("neighbors of region 1: ")
print( dengue_nb[1, ]$nb)
cat("neighbors of region 6: ")
print( dengue_nb[6, ]$nb)

cat("neighbors of region 118: ")
print( dengue_nb[118, ]$nb)
cat("neighbors of region 160: ")
print( dengue_nb[160, ]$nb)
```

```{r}
#gi_stars <- dengue_nb %>% 
#  group_by(epiweek) %>% 
#  mutate(gi_star = local_gstar_perm(
#    total_count, nb, wt)) %>% 
#  tidyr::unnest(gi_star)
```

## **Global Spatial Autocorrelation**

### **Computing Contiguity Spatial Weights**

Similar to the above method, we can calculate the spatial weights separately for global spatial autocorrelation statistics analysis.

```{r}
wm_q <- poly2nb(dengue_df_combined, 
                queen=TRUE)
summary(wm_q)
```

The summary report above shows that there are 3082 area units in Taiwan's selected districts.
There are 19 most connected areas unit has 212 neighbours.

### **Row-standardised weights matrix**

similar to above we can manually assign weights to each neighboring polygon.

```{r}
rswm_q <- nb2listw(wm_q, 
                   style="W", 
                   zero.policy = TRUE)
rswm_q
```

### **Global Spatial Autocorrelation: Moran’s I**

#### Computing Monte Carlo Moran’s I

A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm= moran.mc(dengue_df_combined$total_count, 
                listw=rswm_q, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

### **Visualising Monte Carlo Moran’s I**

We can plot the distribution of the statistical values as a histogram and visualise the results of **Monte Carlo Moran’s I.**

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

## **Global Measures of Spatial Autocorrelation: Geary’s C**

In this section, we will perform Geary’s C statistics testing by using appropriate functions of **spdep** package.

### **Computing Monte Carlo Geary’s C**

The code chunk below performs permutation test for Geary’s C statistic by using [`geary.mc()`](https://r-spatial.github.io/spdep/reference/geary.mc.html) function.

```{r}
set.seed(1234)
bperm=geary.mc(dengue_df_combined$total_count, 
               listw=rswm_q, 
               nsim=999)
bperm
```

#### Visualising the Monte Carlo Geary’s C

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, freq=TRUE, breaks=20, xlab="Simulated Geary c")
abline(v=1, col="red") 
```

### **Computing local Moran’s I**

```{r}
fips <- order(dengue_df_combined$total_count)
localMI <- localmoran(dengue_df_combined$total_count, rswm_q)
head(localMI)
```

#### Mapping the local Moran’s I

```{r}
dengue_df_combined.localMI <- cbind(dengue_df_combined,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

#### Mapping both local Moran’s I values and p-values

The choropleth shows on the left is evidence for both positive and negative Ii values.
However, it is useful to consider the p-values for each of these values, as consider above.

```{r}
localMI.map <- tm_shape(dengue_df_combined.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(dengue_df_combined.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

## **Creating a LISA Cluster Map**

### **Plotting Moran scatterplot**

```{r}
nci <- moran.plot(dengue_df_combined$total_count, rswm_q,
                  labels=as.character(dengue_df_combined$VILLTOWN), 
                  xlab="Count of dengue cases 2023", 
                  ylab="Spatially Lag total count 2023")
```

The counts are not well distributed here at all since most of them at count of 1 on the far left.

### **Preparing LISA map classes**

The code chunks below show the steps to prepare a LISA cluster map.

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
```

Next, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.

```{r}
dengue_df_combined$lag_GDPPC <- lag.listw(rswm_q, dengue_df_combined$total_count)
DV <- dengue_df_combined$lag_GDPPC - mean(dengue_df_combined$lag_GDPPC)  
```

This is follow by centering the local Moran’s around the mean.

```{r}
LM_I <- localMI[,1] - mean(localMI[,1])    
```

Next, we will set a statistical significance level for the local Moran.

```{r}
signif <- 0.05    
```

These four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.

```{r}
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4    
```

Lastly, places non-significant Moran in the category 0.

```{r}
quadrant[localMI[,5]>signif] <- 0
```

In fact, we can combined all the steps into one single code chunk as shown below:

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
dengue_df_combined$lag_GDPPC <- lag.listw(rswm_q, dengue_df_combined$total_count)
DV <- dengue_df_combined$lag_GDPPC - mean(dengue_df_combined$lag_GDPPC)     
LM_I <- localMI[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4    
quadrant[localMI[,5]>signif] <- 0
```

### **Plotting LISA map and** local Moran’s I values

For effective interpretation, it is better to plot both the local Moran’s I values map and LISA map next to each other.

The code chunk below will be used to create such visualisation.

```{r}
total_count <- qtm(dengue_df_combined, "total_count")

dengue_df_combined.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(dengue_df_combined.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(total_count, LISAmap, 
             asp=1, ncol=2)
```

There are a significant number of low-low regions which indicates most of these regions are not heavily affected by dengue.
The light blue clusters showing worrying trends since they may be the origination of dengue spots and may influence surrounding regions to get dengue as well.
Policy makers need to take note of these regions and push for more awareness in these low-high regions in particular.

## **Performing Emerging Hotspot Analysis**

Lets use the spacetime cube generated to find out trends of how the dengue is spreading.
This information can be useful for policy makers in taiwan to set up more posters and awareness campaigns to get people in more vulnerable regions to take action to not only reduce dengue onset after it has happened but also encourage them to take preventive measures so it never happens to them as well.

```{r}
ehsa = emerging_hotspot_analysis(bos, "total_count", threshold = 0.05)
```

Lets look at the distribution of each of these classifications:

```{r}
table(ehsa$classification)
```

Interestingly, there are 2 regions with no patterns detected.
More data may need to be collected by Taiwan policy makers for these regions for them to better understand the situation and take the appropriate action.
These are the two regions:

```{r}
#| eval: false
ehsa[ehsa$classification == "no pattern detected", ]
```

### **Visualising the distribution of EHSA classes**

```{r}
#| eval: false
#ggplot(data = ehsa, aes(x = classification)) + geom_bar()

g <- ggplot(ehsa, aes(x = classification))  
p <-  g + geom_bar() + ggtitle("Classifications") +  xlab("Type of Emerging Hotspot") + ylab("Count")

ggplotly(p) 
```

Lets view the charts here clearly:

![](images/firstPart.png){width="643"}

![](images/secondPart.png){width="624"}

```{r}
taiwan_sf_filtered
```

### **Visualising EHSA**

Now, we can add the polygons data from taiwan_sf_filtered to the ehsa analysis and visualise the different types of clusters spatially.

```{r}
#| eval: false
taiwan_ehsa <- taiwan_sf_filtered %>%
  left_join(ehsa , by = join_by(VILLTOWN == location))
```

Lets plots the map:

```{r}
#| eval: false
ehsa_sig <- taiwan_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(taiwan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

![](images/Screenshot 2024-03-03 at 11.30.23 PM.png)

The most common type of hotspot is oscillating hotspot (indicated by the red regions), which indicates most of the regions were experiencing cold spots before they suddenly became infected with dengue.
This indicates there were less persistent hotspots and the reason for this could be ***lack of awareness and actions taken to prevent dengue*** resulting in the people in that region only taking action once they have started experiencing dengue.
Thus, the appearance of dengue seems "sudden" and classifies this region as oscillating hotspot.
Surprisingly there are no persistent hotspots which indicates that all affected regions quickly took actions to reduce the dengue rate and thus allows them to not be statistically significant hot spot for 90 percent of the time-step intervals.
This is a good sign about the level of awareness for users who have already been infected.

## Reflections

This study has been inspiring since it makes you think about spatiotemporal relationships on a deep level for identifying patterns over time .
This is definitely beneficial for me in the future given that there are few tools that allow use to analyse spatiotemporal relationships much less visualise them.
These spatiotemporal cubes can possibly be used in future applications such as 3D VR applications for non technical policy makers from other industries see these trends better.
Specifically it shows how important it is for policy makers in charge of controlling the spread of these viruses through campaigns.
These analysis will allow them to accurately find which regions to target first and invest more to curb the spread more effectively.

## 

# References

-   <https://r4gdsa.netlify.app/chap04.html>

-   <https://r4gdsa.netlify.app/chap07.html>

-   <https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/learnmoreemerging.htm#:~:text=Oscillating%20Hot%20Spot,been%20statistically%20significant%20hot%20spots.>

## Miscellenous

### Other explorations

#### Computing Gi\*

isolating only one week :

```{r}
dengue_nb_week31 = filter(dengue_nb, dengue_nb$epiweek == 31)
head(dengue_nb_week31)
```

some of them have no neighbors

```{r}
unlist(dengue_nb_week31[1, ]$nb)
```

```{r}
dengue_nb_week31[187, ]$nb
lengths(dengue_nb_week31[187, ]$nb)
print("--------------")
#card(dengue_nb_week31[1, ]$nb[1])
print("--------------")
#card(dengue_nb_week31[187, ]$nb)

```

```{r}
#gi_star = local_gstar_perm( dengue_nb_week31, dengue_nb_week31$nb, dengue_nb_week31$wt)
#gi_star
```
