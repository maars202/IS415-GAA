---
title: "Take Home Exercise 2"
author: "Maaruni"
execute: 
  warning: false
date: 02/09/2024
editor: 
  markdown: 
    wrap: sentence
---

# Overview

## Setting the Scene

Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions.
It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes.
In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths.
Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases.
However, in 2023, Taiwan recorded 26703 dengue fever cases.
Figure below reveals that more than 25,000 cases were reported at Tainan City.

Figure 2 and 3 below reveal that more than 80% of the reported dengue fever cases occurred in the month August-November 2023 and epidemiology week 31-50.

## Objectives

As a curious geospatial analytics green horn, you are interested to discover:

if the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.
If the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

## The Task

The specific tasks of this take-home exercise are as follows:

Using appropriate function of sf and tidyverse, preparing the following geospatial data layer: a study area layer in sf polygon features.
It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.
a dengue fever layer within the study area in sf point features.
The dengue fever cases should be confined to epidemiology week 31-50, 2023.
a derived dengue fever layer in spacetime s3 class of sfdep.
It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.
Using the extracted data, perform global spatial autocorrelation analysis.
Using the extracted data, perform local spatial autocorrelation analysis.
Using the extracted data, perform emerging hotspot analysis.
Describe the spatial patterns revealed by the analysis above.

## The Data

For the purpose of this take-home exercise, two data sets are provided, they are:

TAIWAN_VILLAGE_2020, a geospatial data of village boundary of Taiwan.
It is in ESRI shapefile format.
The data is in Taiwan Geographic Coordinate System.
(Source: Historical map data of the village boundary: TWD97 longitude and latitude)

Dengue_Daily.csv, an aspatial data of reported dengue cases in Taiwan since 1998.
(Source: Dengue Daily Confirmed Cases Since 1998. Below are selected fields that are useful for this study:

發病日: Onset date 最小統計區中心點X: x-coordinate 最小統計區中心點Y: y-coordinate Both data sets have been uploaded on eLearn.
Students are required to download them from eLearn.

# Getting Started

## Data Acquisition

|                                |                                                                           |
|----------------------|--------------------------------------------------|
| [**Dataset Name**]{.underline} | [**Source**]{.underline}                                                  |
| TAIWAN_VILLAGE_2020            | Historical map data of the village boundary: TWD97 longitude and latitude |
| Dengue_Daily.csv               | Dengue Daily Confirmed Cases Since 1998                                   |

## Installing and Loading Packages

Lets install the relevant R libraries needed using pacman.

```{r}
#| eval: false
pacman::p_load(sf, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr, spacetime)
```

### Setting Important Configurations

```{r}
#| eval: false
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

```{r}
#| eval: false
currentdirec = list.files(path="../../data/takehomeassigment2/geospatial", pattern=NULL, all.files=FALSE, 
    full.names=FALSE)
currentdirec
```

Reading the grab aspatial data:

```{r}
#| eval: false
dengue_df <- read_csv("../../data/takehomeassigment2/aspatial/Dengue_Daily.csv")
dengue_df
```

Getting Columns of dengue_df:

```{r}
#| eval: false
names(dengue_df)
```

Lets print a summary of dengue_df to find the data distribution and other info:

```{r}
#| eval: false
summary(dengue_df[])
```

We need to retrieve the coastal outline of Taiwan village so that we are able to fetch the dengue infections specifically within these boundaries.

```{r}
#| eval: false
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE") %>% 
  st_transform(crs = 3826)
taiwan_sf
```

```{r}
#| eval: false
plot(taiwan_sf)
```

According to <https://epsg.io/3826>, the EPSG code for taiwan to be used as the coordinate system is 3826.
Thus, the data has been projected to Taiwan's coordinate system using 3826.

```{r}
#| eval: false
# find location of missing values
print("Position of missing values ")
which(is.na(dengue_df))
 
# count total missing values 
print("Count of total missing values  ")
sum(is.na(dengue_df))


colSums(is.na(dengue_df))
```

::: callout-tip
Interesting observation I had here was how all the columns had no missing values and yet the map was not being plotted.
Then, I realised it was due to the null values being presented as "None" string instead of the numerical value such as the example below.
:::

```{r}
#| eval: false
dengue_df[6,]$最小統計區中心點X
```

Lets filter out all the None values to get valid x and y coordinates.
最小統計區中心點X, 最小統計區中心點Y

```{r}
#| eval: false
dengue_df_filtered <- filter(dengue_df, 最小統計區中心點X != "None" & 最小統計區中心點Y != "None")
dengue_df_filtered
```

|     |
|-----|
|     |
|     |

filter by epiweek 31 to 50 in 2023 - <https://www.cmmcp.org/sites/g/files/vyhlif2966/f/uploads/epiweekcalendar2023.pdf>

```{r}
dengue_df_filtered$epiweek = epiweek(dengue_df_filtered$發病日)
names(dengue_df_filtered)
```

```{r}
start_date = "2023-07-30"
end_date = "2023-12-16"
dengue_df_filtered = filter(dengue_df_filtered, 發病日 >= start_date & 發病日 <= end_date)
dengue_df_filtered
```

todo: After filtering valid points, lets convert the latitude and longitude from TWD97 to wsg84 as it conforms to google maps and most global standards for easier analysis.

<!--#  3826 https://epsg.io/transform#s_srs=3826&t_srs=4326&ops=3830&x=NaN&y=NaN -->

```{r}
#| eval: false
dengue_df_filtered <- st_as_sf(dengue_df_filtered, 
                       coords = c("最小統計區中心點X","最小統計區中心點Y"),
                       crs=3826) %>%
st_transform(crs = 3826)
glimpse(dengue_df_filtered)
```

```{r}
#| eval: false
tmap_mode('view')
tm_shape(dengue_df_filtered[1:100,])+
 tm_dots()
```

最小統計區中心點X, 最小統計區中心點Y

Let us save this combined dataframe to RDS so that we will not need to repeat the above steps.
Replace filepath with the directory you would like to save the rds at.

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
write_rds(dengue_df_filtered, filepath) 
```

Continue from here for subsequent steps:

```{r}
#| eval: false
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
dengue_df 
```

# Analysis

```{r}
pacman::p_load(sf, spNetwork, tmap, classInt, viridis, tidyverse, list, arrow, lubridate, tidyverse, readr, sp, maptools, raster, spatstat, spdep, readr, ggplot2, plotly, hexbin, gganimate, gifski, png, transformr, dplyr)
folderToSave <- "/Users/maarunipandithurai/Documents/maars202/geospatial/IS415-GAA/data/rds"
```

```{r}
taiwan_sf <- st_read(dsn = "../../data/takehomeassigment2/geospatial", layer="TAINAN_VILLAGE")
taiwan_sf
```

```{r}
filepath <- str_interp("${folderToSave}/dengue_df")
dengue_df <- read_rds(filepath, refhook = NULL)
dengue_df 
```

Columns in dengue_df:

```{r}
names(dengue_df)
#感染縣市
```

```{r}
names(dengue_df)[1] = "Day_of_onset"
names(dengue_df)[2] = "Day_of_judgement"
names(dengue_df)[3] = "Day_of_report"
names(dengue_df)[4] = "Gender"
names(dengue_df)[5] = "Age_group"

names(dengue_df)[6] = "County_and_city_of_residence"
names(dengue_df)[7] = "TOWNNAME"
names(dengue_df)[8] = "VILLNAME"
names(dengue_df)[9] = "Minimum_statistical_area"
names(dengue_df)[10] = "First_level_statistical_area"

names(dengue_df)[11] = "Secondary_level_statistical_area"
names(dengue_df)[12] = "Infected_counties_and_cities"
names(dengue_df)[13] = "Infected_towns"
names(dengue_df)[14] = "Infect_the_village"
names(dengue_df)[15] = "Whether_to_immigrate_from_abroad"

names(dengue_df)[16] = "Infected_countries"
names(dengue_df)[17] = "Number_of_cases"
names(dengue_df)[18] = "Residential_village_code"
names(dengue_df)[19] = "Infected_Village_Code"
names(dengue_df)[20] = "Serotype"

names(dengue_df)[21] = "Ministry_of_Interior_county"
names(dengue_df)[22] = "Home_Office_Township_Code_of_Residence"
names(dengue_df)[23] = "Ministry_of_Interior"
names(dengue_df)[24] = "Home_Office_Infection_Township_Code"
```

## Data distribution for columns

::: panel-tabset
#### Age

```{r}
#| eval: false
g <- ggplot(dengue_df_2, aes(年齡層))  
p <-  g + geom_bar() + ggtitle("Count of Entries by Age") +  xlab("Age") + ylab("Count")

ggplotly(p) 
```

#### Infected_counties_and_cities

```{r}
#| eval: false
g <- ggplot(dengue_df, aes(Infected_counties_and_cities))  
p <-  g + geom_bar() + ggtitle("Count of Entries by 感染縣市") +  xlab("Infected_counties_and_cities") + ylab("Count")

ggplotly(p) 


ggplotly(p)

```

\
:::

```{r}
#plots will be done later due to time constraints
#plot(dengue_df)
```

only need county codes:

```{r}
taiwan_sf_filtered <- filter(taiwan_sf, TOWNID %in% c('D01', 'D02', 'D04', 'D06', 'D07', 'D08', 'D32', 'D39'))
taiwan_sf_filtered
```

recode chinese column names to enlgish

```{r}
names(taiwan_sf_filtered)
```

group by village town and week to get the count according to week and region

```{r}
dengue_df_count = dengue_df %>% group_by(VILLNAME, TOWNNAME, epiweek(Day_of_onset)) %>%
  summarise(total_count = n())
dengue_df_count
```

maybe can save here:

```{r}
length(replace(dengue_df_count$VILLNAME, "None", ""))
length(dengue_df_count$VILLNAME)
```

```{r}
#dengue_df_count$VILLNAME[dengue_df_count$VILLNAME == 'None']
#apply(X = is.na(dengue_df_count), MARGIN = 2, FUN = sum)
#apply(X = dengue_df_count$VILLNAME[dengue_df_count$VILLNAME == 'None'], MARGIN = 2, FUN = sum)
#is.na(dengue_df_count)
#dengue_df_count[dengue_df_count$VILLNAME == 'None']
print(length(dengue_df_count$VILLNAME))
print(table(dengue_df_count$VILLNAME[dengue_df_count$VILLNAME == '']))
print(length(taiwan_sf_filtered$VILLNAME))
print(table(taiwan_sf_filtered$VILLNAME[taiwan_sf_filtered$VILLNAME == 'None']))

```

since frequency of "None" and "" is 0 in taiwan_sf_filtered means it has no null values as such, the entries with town = "None" will not be matched to any of the polygons in the geospatila data later during the left join anyway.

```{r}
dengue_df_count$VILLTOWN = paste(dengue_df_count$VILLNAME, dengue_df_count$TOWNNAME)
taiwan_sf_filtered$VILLTOWN = paste(taiwan_sf_filtered$VILLNAME, taiwan_sf_filtered$TOWNNAME)
```

```{r}
dengue_df_count <- st_drop_geometry(dengue_df_count)
dengue_df_count
```

left join to aspatial data to add the attributes of aspatial to regions

```{r}
dengue_df_combined = left_join(taiwan_sf_filtered, dengue_df_count)
dengue_df_combined
```

```{r}
print(nrow(table(dengue_df_combined$VILLNAME)))
print(names(dengue_df_combined))
```

```{r}
dengue_df_combined <- dengue_df_combined %>%
  select(11, 12, 13)
dengue_df_combined
```

checking entries per villtown should be the same periods:

```{r}
dengue_df_distribution = dengue_df_combined %>% group_by(VILLTOWN) %>%
  summarise(total_count_weeks = n())
dengue_df_distribution
```

this has 3082 features \< 20 periods \* 258 lcoatoins = 5160

or is it 253 locations

```{r}
dengue_df_combined <- st_drop_geometry(dengue_df_combined)
dengue_df_combined
```

creating rows for missing weeks for each region:

```{r}
#dengue_df_distribution
#dengue_df_combined
#for each town check if the week exists is not then add row for that week for that reach with count 0 
total_invalids = 0
for(i in 1:nrow(dengue_df_distribution))
{
  region = dengue_df_distribution$VILLTOWN[i]
  cat("region: ", region)
  for(j in 31:50){
    
    #print(length(dengue_df_combined[dengue_df_combined['epiweek(Day_of_onset)'] == j & dengue_df_combined['VILLTOWN'] == region]) != 3)
    
    
    if (length(dengue_df_combined[dengue_df_combined['epiweek(Day_of_onset)'] == j
          & dengue_df_combined['VILLTOWN'] == region]) != 3){
    matching_week_region = c(region,j, 0)
    #print(matching_week_region)
    dengue_df_combined = rbind(dengue_df_combined,matching_week_region) 
    total_invalids = total_invalids + 1
    }
  }
  print(total_invalids)
}
print(total_invalids)
```

```{r}
dengue_df_distribution2 = dengue_df_combined %>% group_by(VILLTOWN) %>%
  summarise(total_count_weeks = n())
#dengue_df_distribution2_NULL = dengue_df_distribution2[dengue_df_distribution2$total_count_weeks != 20, ]
#dengue_df_distribution2 = dengue_df_distribution2[dengue_df_distribution2$total_count_weeks == 20, ]

dengue_df_distribution2
```

```{r}
print(length(taiwan_sf_filtered))
taiwan_sf_filtered
```

```{r}
#dengue_df_combined[dengue_df_combined$VILLTOWN  dengue_df_distribution2$VILLTOWN]
dengue_df_combined_correct_size <- subset(dengue_df_combined, VILLTOWN %in% dengue_df_distribution2$VILLTOWN)
dengue_df_combined_correct_size
```

```{r}
taiwan_sf_filtered_correct_size = subset(taiwan_sf_filtered, VILLTOWN %in% dengue_df_combined_correct_size$VILLTOWN)
taiwan_sf_filtered_correct_size_NULL = subset(taiwan_sf_filtered, VILLTOWN %in% dengue_df_distribution2_NULL$VILLTOWN)

taiwan_sf_filtered_correct_size
```

```{r}
plot(taiwan_sf_filtered_correct_size_NULL)
```

```{r}
#dengue_df_combined[dengue_df_combined$VILLTOWN  dengue_df_distribution2$VILLTOWN]
dengue_df_combined_correct_size <- subset(dengue_df_combined, VILLTOWN %in% taiwan_sf_filtered_correct_size$VILLTOWN)
dengue_df_combined_correct_size
```

```{r}
dengue_df_distribution4 = dengue_df_combined_correct_size %>% group_by(VILLTOWN) %>%
  summarise(total_count_weeks = n())
dengue_df_distribution4
```

224 locations, 20 time periods, how many regions are there?
in the aspatial data (225) and spatial data (224)

we need to remove the one region that is not present in aspatial data for the spacetime cube

4533 \< 5160 – still missing 600?
why not filling up

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse)
```

```{r}
typeof(dengue_df_combined['epiweek(Day_of_onset)'])
print((dengue_df_combined['epiweek(Day_of_onset)']))
#transform(dengue_df_combined['epiweek(Day_of_onset)'], char = as.integer(char))
#sapply(dengue_df_combined['epiweek(Day_of_onset)'], integer)
dengue_df_combined['epiweek(Day_of_onset)'] <- as.integer(unlist(dengue_df_combined['epiweek(Day_of_onset)']))
dengue_df_combined['total_count'] <- as.integer(unlist(dengue_df_combined['total_count']))
dengue_df_combined_correct_size['epiweek(Day_of_onset)'] <- as.integer(unlist(dengue_df_combined_correct_size['epiweek(Day_of_onset)']))
dengue_df_combined_correct_size['total_count'] <- as.integer(unlist(dengue_df_combined_correct_size['total_count']))

dengue_df_combined_correct_size
```

These are towns with no observations.
However, since they exist as neighbors of other cities we need to add dummy observations for all the weeks for each of these towsn as well to get 258 \* 20 = 5160 observations for spacetime cube later:

```{r}
#sum(is.na(dengue_df_combined_correct_size))
dengue_df_combined_correct_size[is.na(dengue_df_combined_correct_size$VILLTOWN), ]
dengue_df_combined_correct_size[is.na(dengue_df_combined_correct_size$`epiweek(Day_of_onset)`), ]$VILLTOWN
```

```{r}
dengue_df_combined_correct_size[dengue_df_combined_correct_size$VILLTOWN == '鹿耳里 安南區', ]
```

```{r}
bos = spacetime(dengue_df_combined_correct_size, taiwan_sf_filtered_correct_size,
                .loc_col = "VILLTOWN",
                .time_col = "epiweek(Day_of_onset)")
```

```{r}
bos
```

```{r}
print(224 * 20)
print(nrow(bos))
```

```{r}
is_spacetime_cube(bos)
```

```{r}
dengue_nb <- bos %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")

print(summary(dengue_nb))
head(dengue_nb)
```

```{r}
vacrate_anim <-
  tm_shape(combined_jkt_vac) + tm_fill("VaccinationRate",
            palette = "Greens") +
    tm_borders(lwd = 0.1) +
  tm_facets(along = "period", free.coords = FALSE)
```

```{r}
row.names(dengue_nb) <- NULL
```

```{r}
cat("neighbors of region 1: ")
print( dengue_nb[1, ]$nb)
cat("neighbors of region 6: ")
print( dengue_nb[6, ]$nb)

cat("neighbors of region 118: ")
print( dengue_nb[118, ]$nb)
cat("neighbors of region 118: ")
print( dengue_nb[118, ]$nb)
```

## **Computing Gi\***

```{r}
dengue_nb['total_count'] <- as.integer(unlist(dengue_nb['total_count']))
dengue_nb
```

```{r}
gistars <- dengue_nb %>%
  group_by('epiweek(Day_of_onset)') %>%
  mutate(gi_star = local_gstar_perm(
    total_count, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

isolating only one week :

```{r}
dengue_nb_week31 = filter(dengue_nb, dengue_nb$`epiweek(Day_of_onset)` == 31)
row.names(dengue_nb) <- NULL
dengue_nb_week31
```

```{r}
#dengue_nb_week31 = filter(dengue_nb_week31, lengths(dengue_nb_week31$nb) != 1)
#row.names(dengue_nb) <- NULL
#dengue_nb_week31
```

some of them have no neighbors

```{r}
lengths(dengue_nb_week31[1, ]$nb)
```

```{r}
dengue_nb_week31[187, ]$nb
lengths(dengue_nb_week31[187, ]$nb)
print("--------------")
card(dengue_nb_week31[1, ]$nb[1])
print("--------------")
card(dengue_nb_week31[187, ]$nb)

```

```{r}
dengue_nb_week31$nb
```

```{r}
gi_star = local_gstar_perm(
    dengue_nb_week31$total_count, dengue_nb_week31$nb, dengue_nb_week31$wt)
```

this might be useful for points that have missing towns:

```         
left_join(dfx, dfy, join_by(closest(a < b))) # similar to above, but only take the closest match
```

find any na values

```{r}

```

add rwos for cities with missing weeks and put 0 - to make spacetime cube later with rihgt periods and values

21 periods and ...locations – mnmo isnt it supposed ot be 20 time periods from 31 to 50th epiweek?

```{r}

```

do an animation of the chanigng counts in all the regions over time similar to /..

```{r}
vacrate_anim <-
  tm_shape(combined_jkt_vac) + tm_fill("VaccinationRate",
            palette = "Greens") +
    tm_borders(lwd = 0.1) +
  tm_facets(along = "period", free.coords = FALSE)
```

for town_vill with na data add 0 and add all teh weeks that are missing – so that the shape of it will match the region column of taiwan_sf_filtered so that space time object can be retreieved

drop st

```{r}

```

::: callout-note
Planning:

-   use statistical means to get distribution and see which one best - identify unique patterns and write reflection on them

<!-- -->

-   add the cold and hot spots as separate layers for users to interact with

-   facet plots

-   make ranges for plots similar when comparing plots side by side
:::

# References

-   <https://r4gdsa.netlify.app/chap04.html>

-   <https://r4gdsa.netlify.app/chap07.html>
